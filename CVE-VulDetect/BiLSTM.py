# ============================================================
# Colab-ready single cell for IoTvulCode training + accuracy curves
# - Mounts Google Drive
# - Auto-finds statement.csv or function.csv
# - Trains TF-IDF + Logistic Regression baseline
# - Trains Keras Embedding + BiLSTM
# - Plots train/val accuracy & loss
# - Saves models & artifacts to Drive
# ============================================================

# =========================
# 0) Install packages
# =========================
!pip install -q joblib scikit-learn matplotlib tensorflow==2.12.0 tqdm

# =========================
# 1) Mount Drive
# =========================
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

# =========================
# 2) Imports
# =========================
import os, glob, json, math, random
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

print("TensorFlow version:", tf.__version__)
device_name = "GPU" if tf.config.list_physical_devices('GPU') else "CPU"
print("Device:", device_name)

# =========================
# 3) Config
# =========================
DATA_DIR = "/content/drive/MyDrive/IoTvulCode-main/IoTvulCode-main/data/iDetectRefine"
OUT_DIR = "/content/drive/MyDrive/IoTvulCode_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

CSV_CANDIDATES = ["statement.csv", "function.csv"]

RANDOM_SEED = 42
TEST_SIZE = 0.15
VAL_SIZE = 0.15
TFIDF_MAX_FEATURES = 50000

TRAIN_KERAS = True
MAX_WORDS = 60000
SEQ_LEN = 300
EMB_DIM = 128
LSTM_UNITS = 128
BATCH_SIZE = 128
EPOCHS = 6

SAMPLE_IF_LARGE = True
LARGE_THRESHOLD = 400_000
NEG_RATIO = 3

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# =========================
# 4) Find CSV
# =========================
data_path = Path(DATA_DIR)
if not data_path.exists():
    raise FileNotFoundError(f"Data directory not found: {DATA_DIR}")

csv_path = None
for c in CSV_CANDIDATES:
    p = data_path / c
    if p.exists():
        csv_path = p
        break

if csv_path is None:
    files = list(data_path.glob("*.csv"))
    if not files:
        raise FileNotFoundError(f"No CSV found in {DATA_DIR}")
    csv_path = files[0]

print("Using CSV:", csv_path)

# =========================
# 5) Detect Columns
# =========================
sample_df = pd.read_csv(csv_path, nrows=5)
print("Columns preview:", sample_df.columns.tolist())

COMMON_TEXT_COLS = ['code','statement','snippet','content','source','function','text','raw']
COMMON_LABEL_COLS = ['label','vulnerable','is_vulnerable','target','y','class','vuln']

def detect_columns(df):
    text_col, label_col = None, None
    for c in df.columns:
        lc = c.lower()
        if not text_col and any(t in lc for t in COMMON_TEXT_COLS):
            text_col = c
        if not label_col and any(l in lc for l in COMMON_LABEL_COLS):
            label_col = c

    if text_col is None:
        for c in df.columns:
            if df[c].dtype == object:
                text_col = c
                break

    if label_col is None:
        for c in df.columns:
            if df[c].nunique(dropna=True) <= 3:
                label_col = c
                break

    return text_col, label_col

text_col, label_col = detect_columns(sample_df)
print("Detected:", text_col, label_col)

# =========================
# 6) Load Full CSV
# =========================
df = pd.read_csv(csv_path, low_memory=False)
print("Shape:", df.shape)

# =========================
# 7) Text Cleaning
# =========================
def simple_code_clean(srs):
    s = srs.fillna('').astype(str)
    s = s.str.replace(r'\r\n|\r|\n', ' ', regex=True)
    s = s.str.replace(r'\s+', ' ', regex=True)
    s = s.str.replace(r'([{}()\[\].,;:+\-*/=&|<>%!~^])', r' \1 ', regex=True)
    return s.str.lower()

df[text_col] = simple_code_clean(df[text_col])

# =========================
# 8) Normalize Labels
# =========================
def normalize_labels(y):
    if y.dtype == object:
        ylow = y.astype(str).str.lower()
        pos_tokens = ['vuln','vulnerable','1','true','yes','y']
        neg_tokens = ['benign','safe','0','false','no','n','clean']
        mapped = []
        for v in ylow.fillna(''):
            if any(tok in v for tok in pos_tokens):
                mapped.append(1)
            elif any(tok in v for tok in neg_tokens):
                mapped.append(0)
            else:
                try:
                    num = float(v)
                    mapped.append(1 if num != 0 else 0)
                except:
                    mapped.append(0)
        return np.array(mapped, dtype=np.int32)
    else:
        ynum = pd.to_numeric(y, errors='coerce').fillna(0)
        return (ynum > 0).astype(int).values

y = normalize_labels(df[label_col])
X = df[text_col].values
print("Class distribution:", dict(zip(*np.unique(y, return_counts=True))))

# =========================
# 9) Sampling if Large
# =========================
if SAMPLE_IF_LARGE and len(X) > LARGE_THRESHOLD:
    pos_idx = np.where(y==1)[0]
    neg_idx = np.where(y==0)[0]

    nneg_keep = min(len(neg_idx), max(5000, len(pos_idx) * NEG_RATIO))
    rng = np.random.RandomState(RANDOM_SEED)
    neg_keep = rng.choice(neg_idx, size=nneg_keep, replace=False)

    keep = np.concatenate([pos_idx, neg_keep])
    X, y = X[keep], y[keep]

# =========================
# 10) Train/Val/Test Split
# =========================
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED
)

val_fraction = VAL_SIZE / (1.0 - TEST_SIZE)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=val_fraction, stratify=y_temp, random_state=RANDOM_SEED
)

# =========================
# 11) TF-IDF + Logistic Regression
# =========================
vect = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1,3))
Xtr = vect.fit_transform(X_train)
Xval = vect.transform(X_val)
Xte  = vect.transform(X_test)

classes = np.unique(y_train)
cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
class_weight = dict(zip(classes, cw))

clf = LogisticRegression(max_iter=200, class_weight=class_weight, n_jobs=-1)
clf.fit(Xtr, y_train)

ytest_pred = clf.predict(Xte)

print("TF-IDF Test F1:", f1_score(y_test, ytest_pred))

joblib.dump(vect, OUT_DIR + "/tfidf_vectorizer.joblib")
joblib.dump(clf, OUT_DIR + "/logreg_tfidf.joblib")

# =========================
# 12) Keras BiLSTM Model
# =========================
if TRAIN_KERAS:
    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='', oov_token='[OOV]')
    tokenizer.fit_on_texts(X_train)

    Xtrain_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=SEQ_LEN)
    Xval_pad   = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=SEQ_LEN)
    Xtest_pad  = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=SEQ_LEN)

    inp = layers.Input(shape=(SEQ_LEN,))
    x = layers.Embedding(MAX_WORDS, EMB_DIM, mask_zero=True)(inp)
    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS))(x)
    x = layers.Dense(64, activation='relu')(x)
    out = layers.Dense(1, activation='sigmoid')(x)

    model = models.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    history = model.fit(
        Xtrain_pad, y_train,
        validation_data=(Xval_pad, y_val),
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        class_weight=class_weight
    )

    model.save(OUT_DIR + "/keras_final.h5")
    joblib.dump(tokenizer, OUT_DIR + "/keras_tokenizer.joblib")

    # =========================
    # 13) Plot Curves
    # =========================
    plt.plot(history.history['accuracy'], label='train')
    plt.plot(history.history['val_accuracy'], label='val')
    plt.legend()
    plt.savefig(OUT_DIR + "/keras_accuracy.png")
    plt.show()

print("âœ… All models and outputs saved to:", OUT_DIR)
