# ==============================================================
# Efficient Hybrid Labeling for C/C++ IoT Code
# ==============================================================

!pip install requests pandas tqdm -q

import pandas as pd
import requests
from tqdm import tqdm
import json

# Load your dataset
CSV_PATH = "/content/drive/MyDrive/IoTvulCode-main/IoTvulCode-main/data/iDetectRefine/DNN-binary.csv"
df = pd.read_csv(CSV_PATH)

# ==============================================================
# 1. CVE VALIDATION - THE CORE IMPROVEMENT
# ==============================================================

def check_cve_for_project(project_name, version):
    """
    Check if a project+version has known CVEs
    Returns: List of CVE IDs if found, empty list if none
    """
    try:
        # Search NVD for CVEs affecting this project version
        url = f"https://services.nvd.nist.gov/rest/json/cves/2.0"
        params = {
            'keywordSearch': project_name,
            'versionStart': version,
            'versionEnd': version
        }
        
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            cves = []
            for vuln in data.get('vulnerabilities', []):
                cve_id = vuln['cve']['id']
                cves.append(cve_id)
            return cves
        return []
    except:
        return []

# ==============================================================
# 2. PROJECT-VERSION MAPPING (You'll need to fill this)
# ==============================================================

# Map your code samples to their project and version
# This is CRITICAL - you need to know which project each code comes from
PROJECT_MAP = {
    # Example: 
    # "sample_1": {"project": "FreeRTOS", "version": "202212.01"},
    # "sample_2": {"project": "RIOT", "version": "2023.07"},
    # You need to create this mapping based on your dataset
}

# ==============================================================
# 3. HYBRID LABELING WITH CVE VALIDATION
# ==============================================================

def get_final_label(row, project_info, original_label):
    """
    Smart labeling using CVE validation + original static analysis
    """
    if project_info:
        project = project_info['project']
        version = project_info['version']
        
        # Check if this project version has known CVEs
        cves = check_cve_for_project(project, version)
        
        if cves:
            # This code comes from a project version with known vulnerabilities
            # More likely that static analysis findings are REAL
            if original_label == 1:  # If static tools said "vulnerable"
                return 1, f"CVE_confirmed:{len(cves)}"
            else:
                return 0, "safe_despite_cves"
        else:
            # No known CVEs for this project version
            # Static analysis findings might be false positives
            if original_label == 1:
                return 0, "likely_false_positive"  # Downgrade to safe
            else:
                return 0, "confirmed_safe"
    
    # Fallback: use original label if no project info
    return original_label, "original_label"

# ==============================================================
# 4. APPLY HYBRID LABELING
# ==============================================================

print("Applying hybrid labeling with CVE validation...")

final_labels = []
label_sources = []

for idx, row in tqdm(df.iterrows(), total=len(df)):
    original_label = row['label']  # Assuming your dataset has a 'label' column
    
    # Get project info - YOU NEED TO POPULATE THIS
    sample_id = f"sample_{idx}"  # Adjust based on your dataset
    project_info = PROJECT_MAP.get(sample_id, None)
    
    final_label, source = get_final_label(row, project_info, original_label)
    
    final_labels.append(final_label)
    label_sources.append(source)

# Add new labels to dataframe
df['hybrid_label'] = final_labels
df['label_source'] = label_sources

# ==============================================================
# 5. QUICK ANALYSIS OF RESULTS
# ==============================================================

print("\n=== LABEL DISTRIBUTION ===")
print("Original labels:")
print(df['label'].value_counts())
print("\nHybrid labels:")
print(df['hybrid_label'].value_counts())
print("\nLabel sources:")
print(df['label_source'].value_counts())

# Calculate how many labels changed
changed = (df['label'] != df['hybrid_label']).sum()
print(f"\nLabels changed: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)")

# ==============================================================
# 6. SAVE IMPROVED DATASET
# ==============================================================

SAVE_PATH = "/content/drive/MyDrive/hybrid_labeled_cve_validated.csv"
df.to_csv(SAVE_PATH, index=False)
print(f"\nSaved improved dataset to: {SAVE_PATH}")

# ==============================================================
# 7. SIMPLE MODEL TRAINING (Optional - to verify improvement)
# ==============================================================

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer

# Simple text-based features
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(df['code'].astype(str))
y = df['hybrid_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train simple model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("\n=== MODEL PERFORMANCE WITH HYBRID LABELS ===")
print(classification_report(y_test, y_pred))

print("Hybrid labeling complete! The key improvement:")
print("✓ Used CVE database to validate which projects actually had vulnerabilities")
print("✓ Reduced false positives by downgrading labels when no CVEs exist")
print("✓ Created more reliable training data for your AI model")