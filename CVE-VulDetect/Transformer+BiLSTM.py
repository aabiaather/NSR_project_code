# ============================================================
# Colab-ready single cell for IoTvulCode training + accuracy curves
# - TF-IDF + Logistic Regression
# - Keras BiLSTM (50 epochs)
# - Transformer CodeBERT (50 epochs)
# - Saves all models & plots to Drive
# ============================================================

# =========================
# 0) Install Packages
# =========================
!pip install -q joblib scikit-learn matplotlib tensorflow==2.12.0 tqdm transformers

# =========================
# 1) Mount Drive
# =========================
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

# =========================
# 2) Imports
# =========================
import os, random
from pathlib import Path
from tqdm import tqdm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Transformer imports
from transformers import AutoTokenizer, TFAutoModel

print("TensorFlow version:", tf.__version__)
device_name = "GPU" if tf.config.list_physical_devices('GPU') else "CPU"
print("Device:", device_name)

# =========================
# 3) Config
# =========================
DATA_DIR = "/content/drive/MyDrive/IoTvulCode-main/IoTvulCode-main/data/iDetectRefine"
OUT_DIR  = "/content/drive/MyDrive/IoTvulCode_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

CSV_CANDIDATES = ["statement.csv", "function.csv"]

RANDOM_SEED = 42
TEST_SIZE = 0.15
VAL_SIZE  = 0.15
TFIDF_MAX_FEATURES = 50000

# Keras BiLSTM
TRAIN_KERAS = True
MAX_WORDS = 60000
SEQ_LEN = 300
EMB_DIM = 128
LSTM_UNITS = 128
BATCH_SIZE = 128
EPOCHS = 50

# Transformer
TRAIN_TRANSFORMER = True
TRANSFORMER_MODEL_NAME = "microsoft/codebert-base"
TRANSFORMER_MAX_LENGTH = 512
TRANSFORMER_BATCH_SIZE = 8
TRANSFORMER_EPOCHS = 50

# Sampling
SAMPLE_IF_LARGE = True
LARGE_THRESHOLD = 400_000
NEG_RATIO = 3

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# =========================
# 4) Load CSV
# =========================
data_path = Path(DATA_DIR)
csv_path = None

for c in CSV_CANDIDATES:
    p = data_path / c
    if p.exists():
        csv_path = p
        break

if csv_path is None:
    files = list(data_path.glob("*.csv"))
    if not files:
        raise FileNotFoundError("No CSV found!")
    csv_path = files[0]

print("Using CSV:", csv_path)

# =========================
# 5) Detect Columns
# =========================
sample_df = pd.read_csv(csv_path, nrows=5)

COMMON_TEXT_COLS  = ['code','statement','snippet','content','source','function','text','raw']
COMMON_LABEL_COLS = ['label','vulnerable','is_vulnerable','target','y','class','vuln']

def detect_columns(df):
    text_col, label_col = None, None
    for c in df.columns:
        lc = c.lower()
        if not text_col and any(t in lc for t in COMMON_TEXT_COLS):
            text_col = c
        if not label_col and any(l in lc for l in COMMON_LABEL_COLS):
            label_col = c
    return text_col, label_col

text_col, label_col = detect_columns(sample_df)
print("Detected:", text_col, label_col)

# =========================
# 6) Load Full CSV
# =========================
df = pd.read_csv(csv_path, low_memory=False)

# =========================
# 7) Clean Code
# =========================
def simple_code_clean(srs):
    s = srs.fillna('').astype(str)
    s = s.str.replace(r'\s+', ' ', regex=True)
    return s.str.lower()

df[text_col] = simple_code_clean(df[text_col])

# =========================
# 8) Normalize Labels
# =========================
def normalize_labels(y):
    y = pd.to_numeric(y, errors='coerce').fillna(0)
    return (y > 0).astype(int).values

y = normalize_labels(df[label_col])
X = df[text_col].values

# =========================
# 9) Sampling if Huge
# =========================
if SAMPLE_IF_LARGE and len(X) > LARGE_THRESHOLD:
    pos_idx = np.where(y == 1)[0]
    neg_idx = np.where(y == 0)[0]
    nneg_keep = min(len(neg_idx), len(pos_idx) * NEG_RATIO)
    neg_keep = np.random.choice(neg_idx, size=nneg_keep, replace=False)
    keep = np.concatenate([pos_idx, neg_keep])
    X, y = X[keep], y[keep]

# =========================
# 10) Train / Val / Test Split
# =========================
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y)
val_fraction = VAL_SIZE / (1.0 - TEST_SIZE)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_fraction, stratify=y_temp)

# =========================
# Metrics Helper
# =========================
def metrics_report(y_true, y_pred):
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred),
        "recall": recall_score(y_true, y_pred),
        "f1": f1_score(y_true, y_pred)
    }

# =========================
# 11) TF-IDF + Logistic Regression
# =========================
vect = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1,3))
Xtr = vect.fit_transform(X_train)
Xte = vect.transform(X_test)

clf = LogisticRegression(max_iter=200)
clf.fit(Xtr, y_train)

ytest_pred = clf.predict(Xte)
tfidf_metrics = metrics_report(y_test, ytest_pred)

joblib.dump(vect, OUT_DIR + "/tfidf_vectorizer.joblib")
joblib.dump(clf,  OUT_DIR + "/logreg_tfidf.joblib")

# =========================
# 12) Keras BiLSTM
# =========================
if TRAIN_KERAS:
    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='')
    tokenizer.fit_on_texts(X_train)

    Xtrain_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=SEQ_LEN)
    Xval_pad   = pad_sequences(tokenizer.texts_to_sequences(X_val),   maxlen=SEQ_LEN)
    Xtest_pad  = pad_sequences(tokenizer.texts_to_sequences(X_test),  maxlen=SEQ_LEN)

    inp = layers.Input(shape=(SEQ_LEN,))
    x = layers.Embedding(MAX_WORDS, EMB_DIM, mask_zero=True)(inp)
    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS))(x)
    x = layers.Dense(64, activation='relu')(x)
    out = layers.Dense(1, activation='sigmoid')(x)

    model = models.Model(inp, out)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    history = model.fit(
        Xtrain_pad, y_train,
        validation_data=(Xval_pad, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE
    )

    ytest_pred_keras = (model.predict(Xtest_pad).ravel() >= 0.5).astype(int)
    keras_metrics = metrics_report(y_test, ytest_pred_keras)

    model.save(OUT_DIR + "/keras_final.h5")
    joblib.dump(tokenizer, OUT_DIR + "/keras_tokenizer.joblib")

# =========================
# 13) Transformer (CodeBERT)
# =========================
if TRAIN_TRANSFORMER:
    tokenizer_t = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)

    def encode(texts):
        return tokenizer_t(texts.tolist(), padding="max_length",
                           truncation=True, max_length=512, return_tensors="tf")

    train_enc = encode(X_train)
    val_enc   = encode(X_val)
    test_enc  = encode(X_test)

    base_model = TFAutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)

    input_ids = layers.Input(shape=(512,), dtype=tf.int32)
    attn_mask = layers.Input(shape=(512,), dtype=tf.int32)
    x = base_model(input_ids, attention_mask=attn_mask).last_hidden_state[:,0,:]
    x = layers.Dense(128, activation="relu")(x)
    out = layers.Dense(1, activation="sigmoid")(x)

    transformer_model = models.Model([input_ids, attn_mask], out)
    transformer_model.compile(
        optimizer=tf.keras.optimizers.Adam(2e-5),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    transformer_model.fit(
        {"input_ids": train_enc["input_ids"], "attention_mask": train_enc["attention_mask"]},
        y_train,
        validation_data=(
            {"input_ids": val_enc["input_ids"], "attention_mask": val_enc["attention_mask"]},
            y_val
        ),
        epochs=TRANSFORMER_EPOCHS,
        batch_size=TRANSFORMER_BATCH_SIZE
    )

    ytest_pred_transformer = (
        transformer_model.predict({"input_ids": test_enc["input_ids"],
                                   "attention_mask": test_enc["attention_mask"]}).ravel() >= 0.5
    ).astype(int)

    transformer_metrics = metrics_report(y_test, ytest_pred_transformer)

    transformer_model.save(OUT_DIR + "/transformer_final.h5")
    tokenizer_t.save_pretrained(OUT_DIR + "/transformer_tokenizer")

# =========================
# 14) Model Comparison
# =========================
comparison_df = pd.DataFrame({
    "TF-IDF": tfidf_metrics,
    "BiLSTM": keras_metrics,
    "Transformer": transformer_metrics
}).T

print("\n=== FINAL MODEL COMPARISON ===")
print(comparison_df.round(4))

comparison_df.to_csv(OUT_DIR + "/model_comparison_50_epochs.csv")
print("\nâœ… ALL MODELS & RESULTS SAVED TO:", OUT_DIR)
